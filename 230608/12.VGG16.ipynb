{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j8HW9Fs3t_jJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aMtlj2tJvE0X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\zino0\\tensorflow_datasets\\cats_vs_dogs\\4.0.0...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee9cac1f04c0407c81dcc330cef4840e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "325af8db031d4aebaaaaf5883c5f9d9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NonMatchingChecksumError",
          "evalue": "Artifact https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip, downloaded to C:\\Users\\zino0\\tensorflow_datasets\\downloads\\down.micr.com_down_3_E_1_3E1C-ECDB-4869-83s65y3LdGwg1Nl3MtI_KASK_7DbbKUE5i78aSRiYcW5Y.zip.tmp.4f8a98a1a7c04b3bae32461ff8231a64\\kagglecatsanddogs_5340.zip, has wrong checksum:\n* Expected: UrlInfo(size=786.67 MiB, checksum='b7974bd00a84a99921f36ee4403f089853777b5ae8d151c76a86e64900334af9', filename='kagglecatsanddogs_5340.zip')\n* Got: UrlInfo(size=9.74 MiB, checksum='fc7c44939c9e52bd6a6d5298c3ecd2e29360895d1bc193a55058fb65419967a7', filename='kagglecatsanddogs_5340.zip')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcats_vs_dogs\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m (train_dataset, validation_dataset), info \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(name\u001b[39m=\u001b[39mdataset_name, split\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain[:80\u001b[39m\u001b[39m%\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain[-20\u001b[39m\u001b[39m%\u001b[39m\u001b[39m:]\u001b[39m\u001b[39m'\u001b[39m), with_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:640\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[39m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m dbuilder \u001b[39m=\u001b[39m _fetch_builder(\n\u001b[0;32m    635\u001b[0m     name,\n\u001b[0;32m    636\u001b[0m     data_dir,\n\u001b[0;32m    637\u001b[0m     builder_kwargs,\n\u001b[0;32m    638\u001b[0m     try_gcs,\n\u001b[0;32m    639\u001b[0m )\n\u001b[1;32m--> 640\u001b[0m _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    642\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m--> 499\u001b[0m   dbuilder\u001b[39m.\u001b[39mdownload_and_prepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs)\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[0;32m    645\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 646\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    647\u001b[0m       dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    648\u001b[0m       download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m    649\u001b[0m   )\n\u001b[0;32m    651\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    652\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    653\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    654\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1498\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1497\u001b[0m   optional_pipeline_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1498\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(  \u001b[39m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m     dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptional_pipeline_kwargs\n\u001b[0;32m   1500\u001b[0m )\n\u001b[0;32m   1501\u001b[0m \u001b[39m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[39m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[39m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m split_generators \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1505\u001b[0m     split_generators\u001b[39m=\u001b[39msplit_generators,\n\u001b[0;32m   1506\u001b[0m     generator_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_examples,\n\u001b[0;32m   1507\u001b[0m     is_beam\u001b[39m=\u001b[39m\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1508\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\image_classification\\cats_vs_dogs.py:75\u001b[0m, in \u001b[0;36mCatsVsDogs._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m---> 75\u001b[0m   path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownload(_URL)\n\u001b[0;32m     77\u001b[0m   \u001b[39m# There is no predefined train/val/test split for this dataset.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m   \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     79\u001b[0m       tfds\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[0;32m     80\u001b[0m           name\u001b[39m=\u001b[39mtfds\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m       ),\n\u001b[0;32m     85\u001b[0m   ]\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:600\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Add progress bar to follow the download state\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_downloader\u001b[39m.\u001b[39mtqdm():\n\u001b[1;32m--> 600\u001b[0m   \u001b[39mreturn\u001b[39;00m _map_promise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, url_or_urls)\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:830\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    831\u001b[0m     \u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39mget(), all_promises\n\u001b[0;32m    832\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[0;32m    830\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m--> 831\u001b[0m     \u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39mget(), all_promises\n\u001b[0;32m    832\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait(timeout \u001b[39mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_settled_value(_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_target_settled_value\u001b[39m(\u001b[39mself\u001b[39m, _raise\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target()\u001b[39m.\u001b[39m_settled_value(_raise)\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     reraise(\u001b[39mtype\u001b[39m(raise_val), raise_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traceback)\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fulfillment_handler0\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\promise\\promise.py:87\u001b[0m, in \u001b[0;36mtry_catch\u001b[1;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_catch\u001b[39m(handler, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     85\u001b[0m     \u001b[39m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         \u001b[39mreturn\u001b[39;00m (handler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     88\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     89\u001b[0m         tb \u001b[39m=\u001b[39m exc_info()[\u001b[39m2\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:407\u001b[0m, in \u001b[0;36mDownloadManager._download.<locals>.<lambda>\u001b[1;34m(dl_result)\u001b[0m\n\u001b[0;32m    401\u001b[0m   future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_downloader\u001b[39m.\u001b[39mdownload(\n\u001b[0;32m    402\u001b[0m       url, download_tmp_dir, verify\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_ssl\n\u001b[0;32m    403\u001b[0m   )\n\u001b[0;32m    405\u001b[0m \u001b[39m# Post-process the result\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39mthen(\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mlambda\u001b[39;00m dl_result: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_or_validate_checksums(  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m    408\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    409\u001b[0m         path\u001b[39m=\u001b[39mdl_result\u001b[39m.\u001b[39mpath,\n\u001b[0;32m    410\u001b[0m         computed_url_info\u001b[39m=\u001b[39mdl_result\u001b[39m.\u001b[39murl_info,\n\u001b[0;32m    411\u001b[0m         expected_url_info\u001b[39m=\u001b[39mexpected_url_info,\n\u001b[0;32m    412\u001b[0m         checksum_path\u001b[39m=\u001b[39mchecksum_path,\n\u001b[0;32m    413\u001b[0m         url_path\u001b[39m=\u001b[39murl_path,\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    415\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:464\u001b[0m, in \u001b[0;36mDownloadManager._register_or_validate_checksums\u001b[1;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[0;32m    453\u001b[0m   checksum_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dl_path(url, computed_url_info\u001b[39m.\u001b[39mchecksum)\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m   \u001b[39m# Eventually validate checksums\u001b[39;00m\n\u001b[0;32m    456\u001b[0m   \u001b[39m# Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m   \u001b[39m#   download). This is expected as it might mean the downloaded file\u001b[39;00m\n\u001b[0;32m    463\u001b[0m   \u001b[39m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m   _validate_checksums(\n\u001b[0;32m    465\u001b[0m       url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    466\u001b[0m       path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m    467\u001b[0m       expected_url_info\u001b[39m=\u001b[39mexpected_url_info,\n\u001b[0;32m    468\u001b[0m       computed_url_info\u001b[39m=\u001b[39mcomputed_url_info,\n\u001b[0;32m    469\u001b[0m       force_checksums_validation\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_force_checksums_validation,\n\u001b[0;32m    470\u001b[0m   )\n\u001b[0;32m    472\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rename_and_get_final_dl_path(\n\u001b[0;32m    473\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    474\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m     url_path\u001b[39m=\u001b[39murl_path,\n\u001b[0;32m    479\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\zino0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:808\u001b[0m, in \u001b[0;36m_validate_checksums\u001b[1;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    797\u001b[0m     expected_url_info\n\u001b[0;32m    798\u001b[0m     \u001b[39mand\u001b[39;00m computed_url_info\n\u001b[0;32m    799\u001b[0m     \u001b[39mand\u001b[39;00m expected_url_info \u001b[39m!=\u001b[39m computed_url_info\n\u001b[0;32m    800\u001b[0m ):\n\u001b[0;32m    801\u001b[0m   msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    802\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArtifact \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m, downloaded to \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m, has wrong checksum:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    803\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m* Expected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_url_info\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mhttps://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    807\u001b[0m   )\n\u001b[1;32m--> 808\u001b[0m   \u001b[39mraise\u001b[39;00m NonMatchingChecksumError(msg)\n",
            "\u001b[1;31mNonMatchingChecksumError\u001b[0m: Artifact https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip, downloaded to C:\\Users\\zino0\\tensorflow_datasets\\downloads\\down.micr.com_down_3_E_1_3E1C-ECDB-4869-83s65y3LdGwg1Nl3MtI_KASK_7DbbKUE5i78aSRiYcW5Y.zip.tmp.4f8a98a1a7c04b3bae32461ff8231a64\\kagglecatsanddogs_5340.zip, has wrong checksum:\n* Expected: UrlInfo(size=786.67 MiB, checksum='b7974bd00a84a99921f36ee4403f089853777b5ae8d151c76a86e64900334af9', filename='kagglecatsanddogs_5340.zip')\n* Got: UrlInfo(size=9.74 MiB, checksum='fc7c44939c9e52bd6a6d5298c3ecd2e29360895d1bc193a55058fb65419967a7', filename='kagglecatsanddogs_5340.zip')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
          ]
        }
      ],
      "source": [
        "dataset_name = 'cats_vs_dogs'\n",
        "(train_dataset, validation_dataset), info = tfds.load(name=dataset_name, split=('train[:80%]', 'train[-20%:]'), with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lnzVwCFvg4A",
        "outputId": "ebdc1aba-2957-474c-b692-fc36d6b967ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18610"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7NEWI4kvmbW",
        "outputId": "ccc8e88a-38ce-4388-b376-05ffa4c13f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4652"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBWCs4OMvovW",
        "outputId": "6c7f5523-58a3-45ed-c3ca-eb65e7e9fb13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': <tf.Tensor: shape=(262, 350, 3), dtype=uint8, numpy=\n",
              " array([[[242, 248, 248],\n",
              "         [240, 246, 246],\n",
              "         [235, 239, 238],\n",
              "         ...,\n",
              "         [188, 174, 127],\n",
              "         [145, 133,  85],\n",
              "         [161, 149, 101]],\n",
              " \n",
              "        [[238, 244, 244],\n",
              "         [239, 245, 245],\n",
              "         [235, 239, 238],\n",
              "         ...,\n",
              "         [188, 176, 128],\n",
              "         [165, 153, 105],\n",
              "         [178, 168, 119]],\n",
              " \n",
              "        [[237, 241, 240],\n",
              "         [238, 242, 241],\n",
              "         [232, 236, 235],\n",
              "         ...,\n",
              "         [200, 187, 142],\n",
              "         [201, 191, 142],\n",
              "         [200, 192, 143]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[130, 122,  75],\n",
              "         [152, 146,  98],\n",
              "         [154, 148,  98],\n",
              "         ...,\n",
              "         [161, 166, 136],\n",
              "         [ 77,  86,  57],\n",
              "         [ 71,  82,  52]],\n",
              " \n",
              "        [[167, 159, 112],\n",
              "         [105,  99,  51],\n",
              "         [148, 142,  92],\n",
              "         ...,\n",
              "         [ 67,  73,  45],\n",
              "         [ 46,  56,  29],\n",
              "         [ 41,  54,  26]],\n",
              " \n",
              "        [[138, 130,  83],\n",
              "         [ 83,  77,  29],\n",
              "         [135, 129,  79],\n",
              "         ...,\n",
              "         [ 68,  74,  46],\n",
              "         [ 14,  27,   1],\n",
              "         [ 63,  77,  51]]], dtype=uint8)>,\n",
              " 'image/filename': <tf.Tensor: shape=(), dtype=string, numpy=b'PetImages/Dog/10396.jpg'>,\n",
              " 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(train_dataset)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT7B3rpTwj77"
      },
      "outputs": [],
      "source": [
        "def normalize(images):\n",
        "    # 0~1 사이의 값으로 Normalize 합니다.\n",
        "    img, lbl = tf.cast(images['image'], tf.float32) / 255.0, images['label']\n",
        "    # 300 X 300 사이즈로 resize 합니다.\n",
        "    img = tf.image.resize(img, size=(300, 300))\n",
        "    return img, lbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk73SM-Kx5YE"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "epochs=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51YiC_iNv7I3"
      },
      "outputs": [],
      "source": [
        "train_data = train_dataset.map(normalize).batch(BATCH_SIZE).repeat().prefetch(1)\n",
        "valid_data = validation_dataset.map(normalize).batch(BATCH_SIZE).repeat().prefetch(1)\n",
        "steps_per_epoch= int(len(list(train_dataset)) * 0.8) // BATCH_SIZE + 1\n",
        "validation_steps= int(len(list(validation_dataset)) * 0.2) // BATCH_SIZE + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhNkjtHwzFcV"
      },
      "outputs": [],
      "source": [
        "transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsxSGYHJzbtW"
      },
      "outputs": [],
      "source": [
        "transfer_model.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QEvZeK7zkii"
      },
      "outputs": [],
      "source": [
        "model_tr = Sequential([\n",
        "    transfer_model,\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WStkftz8zxsi"
      },
      "outputs": [],
      "source": [
        "model_tr.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM-UCXCy0VvC"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "log_dir = \"/logs/fit/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCtrt-VH0jKY"
      },
      "outputs": [],
      "source": [
        "transfer_history = model_tr.fit(train_data,\n",
        "                             validation_data=(valid_data),\n",
        "                             steps_per_epoch=steps_per_epoch,\n",
        "                             validation_steps=validation_steps,\n",
        "                             epochs=20, callbacks=[tensorboard_callback]\n",
        "                            )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
